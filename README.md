# Chat Suggestions

Проект карточки объявления автомобиля с функционалом предложения вопросов продавцу.

## Описание

Приложение представляет карточку объявления о продаже автомобиля, содержащую:

- Карусель изображений автомобиля
- Кнопки "Позвонить" и "Написать" 
- Блок характеристик автомобиля
- Описание автомобиля
- Блок "Спросите у продавца" с шаблонными вопросами и возможностью множественного выбора

## Технологии

- **Frontend**: React 18, Material-UI (MUI)
- **Backend**: Python Flask, Flask-CORS
- **Контейнеризация**: Docker, Docker Compose

## Запуск проекта локально

### Требования

- Docker
- Docker Compose

### Инструкция

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd chat_suggestions
```

2. (Опционально) Настройте переменные окружения для LLM:
```bash
# Создайте файл .env в корне проекта
echo "OPENAI_API_KEY=your_api_key_here" > .env
echo "OPENAI_MODEL_NAME=gpt-3.5-turbo" >> .env
```

3. Запустите проект с помощью Docker Compose:
```bash
docker-compose up --build
```

4. Откройте браузер и перейдите по адресу:
```
Frontend: http://localhost:3000
Backend API: http://localhost:5000
```

### Остановка проекта

Для остановки контейнеров:
```bash
docker-compose down
```

## Структура проекта

```
chat_suggestions/
├── frontend/                 # React приложение
│   ├── public/
│   │   └── images/          # Изображения автомобилей
│   ├── src/
│   │   ├── components/      # React компоненты
│   │   │   ├── ProductCard.js      # Основная карточка товара
│   │   │   ├── ImageCarousel.js    # Карусель изображений
│   │   │   └── SellerQuestions.js  # Блок вопросов продавцу
│   │   └── App.js          # Главный компонент
│   ├── Dockerfile
│   └── package.json
├── backend/                  # Python Flask API
│   ├── app.py              # Основной файл приложения
│   ├── requirements.txt    # Зависимости Python
│   └── Dockerfile          # Docker конфигурация
├── docker-compose.yml
└── README.md
```

## Функциональность

### Карусель изображений
- Отображение изображений размером 374x328px со скруглением 3px
- Навигация стрелками влево/вправо
- Индикаторы текущего изображения

### Кнопки действий
- **Позвонить**: зеленая кнопка (RGB 2, 209, 92)
- **Написать**: синяя кнопка (RGB 0, 170, 255)

### Блок вопросов продавцу

#### Системные шаблоны
Содержит 4 системных шаблонных вопроса:
1. "Еще продаете?" → "Здравствуйте! Еще продаете автомобиль?"
2. "Торг уместен?" → "Здравствуйте! Скажите, торг уместен?"
3. "Когда можно посмотреть?" → "Здравствуйте! Когда можно посмотреть автомобиль?"
4. "Пришлете видео?" → "Здравствуйте! Можете показать на видео, как выглядит автомобиль?"

#### Функциональность множественного выбора
- **Одиночный выбор**: При клике на одну кнопку текст сразу вставляется в поле
- **Множественный выбор**: При выборе нескольких кнопок отправляется запрос на backend для комбинирования сообщений
- **Интеграция с LLM**: Используется OpenAI API для создания естественных сообщений из шаблонов
- **Fallback логика**: При недоступности LLM используется простое объединение сообщений
- **Визуальная обратная связь**: Выбранные кнопки подсвечиваются синим цветом
- **Debouncing**: Запросы на backend отправляются с задержкой 1 секунда для оптимизации
- **Loader**: Показывается индикатор загрузки во время обработки запроса

#### Пользовательские шаблоны
- Возможность создания до 5 пользовательских шаблонов
- Сохранение в localStorage браузера
- Возможность удаления пользовательских шаблонов

## API Endpoints

### Backend API

#### POST /chat/suggests/combine
Комбинирует несколько шаблонных сообщений в одно с использованием LLM или fallback логики.

**Request Body:**
```json
{
  "messages": ["Еще продаете автомобиль?", "Торг уместен?", "Когда можно посмотреть?"]
}
```

**Response:**
```json
{
  "combined_message": "Здравствуйте! Еще продаете автомобиль? Скажите, торг уместен? И когда можно посмотреть машину?",
  "original_count": 3,
  "used_llm": true
}
```

**Параметры ответа:**
- `combined_message` - объединенное сообщение
- `original_count` - количество исходных сообщений
- `used_llm` - использовался ли LLM для объединения (true/false)

#### GET /health
Проверка работоспособности сервера и доступности LLM.

**Response:**
```json
{
  "status": "OK",
  "message": "Backend работает",
  "llm_available": true
}
```

**Параметры ответа:**
- `llm_available` - доступность LLM для обработки запросов (true/false)

## Автомобили

Приложение случайным образом выбирает один из трех автомобилей:
1. **Mercedes-Benz GL-Class (X166)** - 2014 год, дизель 3.0л, полный привод
2. **Volkswagen Jetta VI** - 2013 год, бензин 1.6л, передний привод  
3. **KIA Sorento (4G)** - 2022 год, бензин 2.5л, полный привод

## Разработка

### Переменные окружения

**Frontend (.env в папке frontend):**
```
REACT_APP_BACKEND_URL=http://localhost:5000
```

**Backend:**
```
FLASK_ENV=development
FLASK_DEBUG=1
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL_NAME=gpt-3.5-turbo
```

**Настройка OpenAI API:**
Для работы с LLM необходимо установить переменную `OPENAI_API_KEY`. Если ключ не установлен, система автоматически переключится на fallback режим (простое объединение сообщений).

**Принцип работы LLM интеграции:**
- При выборе нескольких шаблонов система отправляет их в OpenAI API
- LLM создает естественное сообщение, объединяя все вопросы
- Если LLM недоступен, используется fallback - простое объединение через пробел
- Поле `used_llm` в ответе API показывает, какой метод был использован

**Логирование:**
Backend выводит подробные логи в консоль при запуске и работе:
- Статус инициализации OpenAI клиента
- Доступность LLM при запуске приложения
- Информацию о каждом запросе на комбинирование сообщений
- Использование LLM или fallback метода
- Ошибки при работе с LLM API

Пример логов при запуске:
```
2024-01-15 10:30:00,123 - __main__ - INFO - Запуск Flask приложения
2024-01-15 10:30:00,125 - __main__ - INFO - OPENAI_API_KEY установлен: Да
2024-01-15 10:30:00,126 - __main__ - INFO - OPENAI_API_BASE_URL: https://api.vsegpt.ru/v1
2024-01-15 10:30:00,127 - __main__ - INFO - OPENAI_MODEL_NAME: google/gemini-2.5-flash-pre
2024-01-15 10:30:00,128 - __main__ - INFO - Инициализация OpenAI клиента с base_url: https://api.vsegpt.ru/v1
2024-01-15 10:30:00,350 - __main__ - INFO - OpenAI клиент успешно инициализирован
2024-01-15 10:30:00,351 - __main__ - INFO - ✅ LLM готов к работе
```

### Настройка CORS
Backend настроен на прием запросов от фронтенда с помощью Flask-CORS.

### Hot Reload
Оба сервиса поддерживают автоматическое обновление при изменении кода благодаря volume mapping в Docker Compose.
